{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2004099f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# WTKO RNA Velocity + Contrastive Learning Pipeline\n",
    "# Usage Example Notebook\n",
    "# \n",
    "# SETUP INSTRUCTIONS:\n",
    "# 1. Update 'project_path' to point to your project directory containing models.py, trainers.py, etc.\n",
    "# 2. Update 'adata_path' to point to your h5ad data file\n",
    "# 3. Update 'murk_path' to point to your MURK genes CSV file (optional)\n",
    "# 4. Ensure all required packages are installed (see requirements.txt)\n",
    "# =============================================================================\n",
    "\n",
    "# =============================================================================\n",
    "# 1. Import Required Libraries\n",
    "# =============================================================================\n",
    "\n",
    "# --- Basic Libraries ---\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import sparse\n",
    "from scipy.stats import binned_statistic_2d\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# --- Visualization Libraries ---\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib.colors import Normalize\n",
    "import seaborn as sns\n",
    "\n",
    "# --- Machine Learning & Deep Learning ---\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.optim as optim\n",
    "\n",
    "# --- Single-cell Analysis Libraries ---\n",
    "import scanpy as sc\n",
    "import scvelo as scv\n",
    "import anndata\n",
    "\n",
    "# --- RNA Velocity Specialized Library ---\n",
    "from velovi import VELOVI\n",
    "\n",
    "# --- Dimensionality Reduction & Visualization ---\n",
    "import umap\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# --- Distance & Similarity Computation ---\n",
    "import scipy.spatial.distance as dist\n",
    "from sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score\n",
    "\n",
    "# --- Statistical Analysis ---\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "\n",
    "# --- Progress Bar (Optional) ---\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- Settings ---\n",
    "# scanpy settings\n",
    "sc.settings.verbosity = 3  # verbosity level\n",
    "sc.settings.set_figure_params(dpi=80, facecolor='white')\n",
    "\n",
    "# scvelo settings  \n",
    "scv.settings.verbosity = 3\n",
    "scv.settings.presenter_view = True\n",
    "scv.set_figure_params('scvelo')\n",
    "\n",
    "# matplotlib settings\n",
    "plt.rcParams['figure.figsize'] = (10, 8)\n",
    "plt.rcParams['font.size'] = 12\n",
    "plt.rcParams['axes.titlesize'] = 14\n",
    "plt.rcParams['axes.labelsize'] = 12\n",
    "plt.rcParams['xtick.labelsize'] = 10\n",
    "plt.rcParams['ytick.labelsize'] = 10\n",
    "plt.rcParams['legend.fontsize'] = 10\n",
    "\n",
    "# PyTorch settings\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)\n",
    "\n",
    "# --- Import Models, Trainers, and Data Loaders ---\n",
    "import os\n",
    "import sys\n",
    "# Add directory containing .py files to Python path\n",
    "# Replace 'path/to/your/project' with the actual path to your project directory\n",
    "project_path = 'path/to/your/project'  # Update this path\n",
    "sys.path.append(project_path)\n",
    "\n",
    "# Import modules\n",
    "from models import WTKOContrastiveVAE\n",
    "from trainers import WTKOTrainer  \n",
    "from data import create_wt_ko_dataloaders\n",
    "from utils import plot_latent_space, plot_combined_latent_space\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"WTKO RNA Velocity + Contrastive Learning Pipeline\")\n",
    "print(\"All libraries imported successfully\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f38e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 2. Load and Preprocess Data\n",
    "# =============================================================================\n",
    "\n",
    "# Path to the data file\n",
    "# Replace with your actual data file path\n",
    "adata_path = \"path/to/your/data.h5ad\"  # Update this path\n",
    "\n",
    "# Load AnnData object\n",
    "adata = sc.read_h5ad(adata_path)\n",
    "\n",
    "# Display data overview\n",
    "print(\"Data overview:\")\n",
    "print(adata)\n",
    "print(\"obs columns:\", adata.obs.columns.tolist())\n",
    "\n",
    "# Define blood subclusters to extract\n",
    "blood_subclusters = [\n",
    "    \"BP1\", \"BP2\", \"BP3\", \"BP4\",          # Blood Progenitors\n",
    "    \"Ery1\", \"Ery2\", \"Ery3\", \"Ery4\",      # Erythroid cells\n",
    "    \"Haem1\", \"Haem2\", \"Haem3\", \"Haem4\",  # Hematopoietic cells\n",
    "    \"Mk\", \"My\", \"EC\"                      # Megakaryocytes, Myeloid, Endothelial cells\n",
    "]\n",
    "\n",
    "# Filter for blood subclusters only\n",
    "adata_blood = adata[adata.obs[\"haem_subclust_grouped\"].isin(blood_subclusters)].copy()\n",
    "\n",
    "# Check results\n",
    "print(f\"Number of blood subcluster cells: {adata_blood.n_obs}\")\n",
    "print(\"haem_subclust_grouped distribution after filtering:\")\n",
    "print(adata_blood.obs[\"haem_subclust_grouped\"].value_counts())\n",
    "\n",
    "# Extract KO cells (tomato positive)\n",
    "adata_ko = adata_blood[adata_blood.obs[\"tomato\"] == \"pos\"].copy()\n",
    "\n",
    "# Extract WT cells (tomato negative)\n",
    "adata_wt = adata_blood[adata_blood.obs[\"tomato\"] == \"neg\"].copy()\n",
    "\n",
    "# Check cell counts\n",
    "print(f\"KO cell count: {adata_ko.n_obs}\")\n",
    "print(f\"WT cell count: {adata_wt.n_obs}\")\n",
    "\n",
    "# === Remove MURK genes ===\n",
    "# Path to MURK genes file (mitochondrial and ribosomal genes)\n",
    "# Replace with your actual MURK genes file path\n",
    "murk_path = \"path/to/murk_genes.csv\"  # Update this path\n",
    "murk_genes = pd.read_csv(murk_path, header=None)[0].tolist()\n",
    "\n",
    "# Remove MURK genes from each group (only consider existing genes)\n",
    "valid_murk_genes_ko = [g for g in murk_genes if g in adata_ko.var_names]\n",
    "valid_murk_genes_wt = [g for g in murk_genes if g in adata_wt.var_names]\n",
    "\n",
    "adata_ko = adata_ko[:, [g for g in adata_ko.var_names if g not in valid_murk_genes_ko]].copy()\n",
    "adata_wt = adata_wt[:, [g for g in adata_wt.var_names if g not in valid_murk_genes_wt]].copy()\n",
    "\n",
    "print(f\"KO gene count after MURK removal: {adata_ko.n_vars}\")\n",
    "print(f\"WT gene count after MURK removal: {adata_wt.n_vars}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9317b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 3. VELOVI Processing Functions\n",
    "# =============================================================================\n",
    "\n",
    "def apply_velovi_to_group(adata, group_name, max_epochs=10, min_r2=-10, gamma=-10):\n",
    "    \"\"\"\n",
    "    Apply VELOVI to calculate velocity and return filtered AnnData\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    adata : AnnData\n",
    "        Input single-cell data\n",
    "    group_name : str\n",
    "        Name of the group (for logging)\n",
    "    max_epochs : int\n",
    "        Maximum training epochs for VELOVI\n",
    "    min_r2 : float\n",
    "        Minimum RÂ² threshold for gene filtering\n",
    "    gamma : float\n",
    "        Minimum gamma threshold for gene filtering\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    filtered_adata : AnnData\n",
    "        Filtered data with velocity estimates\n",
    "    model : VELOVI\n",
    "        Trained VELOVI model\n",
    "    \"\"\"\n",
    "    print(f\"\\n=== Processing {group_name} with VELOVI ===\")\n",
    "\n",
    "    # Create copy to preserve original data\n",
    "    adata_copy = adata.copy()\n",
    "\n",
    "    # scVelo preprocessing\n",
    "    scv.pp.filter_and_normalize(adata_copy, min_shared_counts=20, n_top_genes=40000, enforce=True)\n",
    "    scv.pp.moments(adata_copy, n_neighbors=30, n_pcs=50, method='umap')\n",
    "\n",
    "    # Initial velocity estimation (for filtering metrics)\n",
    "    scv.tl.velocity(\n",
    "        adata_copy,\n",
    "        mode=\"dynamical\",\n",
    "        min_r2=min_r2,\n",
    "        gamma=gamma,\n",
    "        filter_genes=False,\n",
    "        use_highly_variable=False\n",
    "    )\n",
    "\n",
    "    # Define gene filtering conditions\n",
    "    mask = (adata_copy.var.velocity_r2 > min_r2) & (adata_copy.var.velocity_gamma > gamma)\n",
    "\n",
    "    print(f\"Original gene count: {adata_copy.n_vars}\")\n",
    "    print(f\"velocity_r2 > {min_r2}: {np.sum(adata_copy.var.velocity_r2 > min_r2)}\")\n",
    "    print(f\"gamma > {gamma}: {np.sum(adata_copy.var.velocity_gamma > gamma)}\")\n",
    "    print(f\"Genes meeting both criteria: {mask.sum()}\")\n",
    "\n",
    "    # Apply filtering\n",
    "    filtered_adata = adata_copy[:, mask].copy()\n",
    "\n",
    "    # VELOVI setup and training\n",
    "    VELOVI.setup_anndata(filtered_adata, spliced_layer=\"Ms\", unspliced_layer=\"Mu\")\n",
    "    model = VELOVI(filtered_adata)\n",
    "    model.train(max_epochs=max_epochs)\n",
    "\n",
    "    # Store velocity estimates\n",
    "    velocity_estimates = model.get_velocity()\n",
    "    filtered_adata.layers[\"velocity\"] = velocity_estimates\n",
    "\n",
    "    # Calculate velocity graph\n",
    "    scv.tl.velocity_graph(filtered_adata)\n",
    "\n",
    "    # Preserve obsm data (UMAP, etc.)\n",
    "    for key in adata.obsm.keys():\n",
    "        if key not in filtered_adata.obsm:\n",
    "            filtered_adata.obsm[key] = adata.obsm[key]\n",
    "\n",
    "    return filtered_adata, model\n",
    "\n",
    "def align_gene_sets(adata_wt, adata_ko):\n",
    "    \"\"\"\n",
    "    Align gene sets between WT and KO AnnData objects\n",
    "    Only use genes that exist in both datasets\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    adata_wt, adata_ko : AnnData\n",
    "        WT and KO datasets\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    wt_aligned, ko_aligned : AnnData\n",
    "        Datasets with aligned gene sets\n",
    "    \"\"\"\n",
    "    print(\"\\n=== Aligning Gene Sets ===\")\n",
    "    \n",
    "    # Get current gene sets\n",
    "    wt_genes = list(adata_wt.var_names)\n",
    "    ko_genes = list(adata_ko.var_names)\n",
    "    \n",
    "    print(f\"WT gene count: {len(wt_genes)}\")\n",
    "    print(f\"KO gene count: {len(ko_genes)}\")\n",
    "    \n",
    "    # Identify common genes\n",
    "    common_genes = list(set(wt_genes).intersection(set(ko_genes)))\n",
    "    print(f\"Common gene count: {len(common_genes)}\")\n",
    "    \n",
    "    if len(common_genes) == 0:\n",
    "        print(\"Warning: No common genes found between WT and KO\")\n",
    "        # Use a subset for minimal processing\n",
    "        common_genes = wt_genes[:min(len(wt_genes), 100)]\n",
    "    \n",
    "    # Filter to common genes only\n",
    "    print(\"Filtering to common genes...\")\n",
    "    wt_aligned = adata_wt[:, common_genes].copy()\n",
    "    ko_aligned = adata_ko[:, common_genes].copy()\n",
    "    \n",
    "    print(f\"Aligned WT gene count: {wt_aligned.n_vars}\")\n",
    "    print(f\"Aligned KO gene count: {ko_aligned.n_vars}\")\n",
    "    \n",
    "    return wt_aligned, ko_aligned\n",
    "\n",
    "def process_and_align_data(adata_wt, adata_ko):\n",
    "    \"\"\"\n",
    "    Main function for VELOVI application and gene set alignment\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    adata_wt, adata_ko : AnnData\n",
    "        WT and KO datasets\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    adata_wt_aligned, adata_ko_aligned : AnnData\n",
    "        Processed and aligned datasets\n",
    "    model_wt, model_ko : VELOVI\n",
    "        Trained VELOVI models\n",
    "    \"\"\"\n",
    "    # Apply VELOVI\n",
    "    print(\"Applying VELOVI to WT group...\")\n",
    "    adata_wt_filtered, model_wt = apply_velovi_to_group(adata_wt.copy(), \"WT\")\n",
    "    \n",
    "    print(\"Applying VELOVI to KO group...\")\n",
    "    adata_ko_filtered, model_ko = apply_velovi_to_group(adata_ko.copy(), \"KO\")\n",
    "    \n",
    "    # Align gene sets\n",
    "    print(\"Aligning gene sets...\")\n",
    "    adata_wt_aligned, adata_ko_aligned = align_gene_sets(\n",
    "        adata_wt_filtered, adata_ko_filtered\n",
    "    )\n",
    "    \n",
    "    return adata_wt_aligned, adata_ko_aligned, model_wt, model_ko\n",
    "\n",
    "# =============================================================================\n",
    "# 4. Process Data with VELOVI\n",
    "# =============================================================================\n",
    "\n",
    "# Main processing\n",
    "adata_wt_aligned, adata_ko_aligned, model_wt, model_ko = process_and_align_data(adata_wt, adata_ko)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8e7ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 5. Prepare Data for Contrastive VAE\n",
    "# =============================================================================\n",
    "\n",
    "def prepare_data_for_contrastive_vae(adata_wt, adata_ko):\n",
    "    \"\"\"\n",
    "    Prepare data for VAE model\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    adata_wt, adata_ko : AnnData\n",
    "        WT and KO datasets\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict : Data dictionary containing arrays and metadata\n",
    "    \"\"\"\n",
    "    print(\"Preparing data for VAE model...\")\n",
    "\n",
    "    # Use haem_subclust_grouped as labels\n",
    "    wt_labels_raw = adata_wt.obs['haem_subclust_grouped'].astype(str)\n",
    "    ko_labels_raw = adata_ko.obs['haem_subclust_grouped'].astype(str)\n",
    "\n",
    "    # Encode common labels\n",
    "    cell_types = sorted(set(wt_labels_raw.unique()) | set(ko_labels_raw.unique()))\n",
    "    cell_type_to_idx = {ct: i for i, ct in enumerate(cell_types)}\n",
    "\n",
    "    # Prepare WT data\n",
    "    wt_data = adata_wt.X.toarray() if sparse.issparse(adata_wt.X) else adata_wt.X\n",
    "    wt_labels = np.array([cell_type_to_idx[ct] for ct in wt_labels_raw])\n",
    "\n",
    "    # Prepare KO data\n",
    "    ko_data = adata_ko.X.toarray() if sparse.issparse(adata_ko.X) else adata_ko.X\n",
    "    ko_labels = np.array([cell_type_to_idx[ct] for ct in ko_labels_raw])\n",
    "\n",
    "    # Prepare velocity data (if available)\n",
    "    if 'velocity' in adata_wt.layers and 'velocity' in adata_ko.layers:\n",
    "        wt_velocity = adata_wt.layers['velocity'].toarray() if sparse.issparse(adata_wt.layers['velocity']) else adata_wt.layers['velocity']\n",
    "        ko_velocity = adata_ko.layers['velocity'].toarray() if sparse.issparse(adata_ko.layers['velocity']) else adata_ko.layers['velocity']\n",
    "    else:\n",
    "        print(\"Warning: velocity data not found\")\n",
    "        wt_velocity = None\n",
    "        ko_velocity = None\n",
    "\n",
    "    print(f\"WT data shape: {wt_data.shape}, KO data shape: {ko_data.shape}\")\n",
    "    print(f\"Common cell types: {set([cell_types[i] for i in set(wt_labels) & set(ko_labels)])}\")\n",
    "\n",
    "    return {\n",
    "        'wt_data': wt_data,\n",
    "        'wt_labels': wt_labels,\n",
    "        'ko_data': ko_data,\n",
    "        'ko_labels': ko_labels,\n",
    "        'wt_velocity': wt_velocity,\n",
    "        'ko_velocity': ko_velocity,\n",
    "        'cell_types': cell_types,\n",
    "        'cell_type_to_idx': cell_type_to_idx\n",
    "    }\n",
    "\n",
    "# Prepare data\n",
    "data_dict = prepare_data_for_contrastive_vae(adata_wt_aligned, adata_ko_aligned)\n",
    "\n",
    "# =============================================================================\n",
    "# 6. Create Data Loaders\n",
    "# =============================================================================\n",
    "\n",
    "def create_dataloaders(data_dict, batch_size=64):\n",
    "    \"\"\"\n",
    "    Create PyTorch DataLoader objects\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    data_dict : dict\n",
    "        Data dictionary from prepare_data_for_contrastive_vae\n",
    "    batch_size : int\n",
    "        Batch size for training\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    wt_loader, ko_loader : DataLoader\n",
    "        PyTorch data loaders for WT and KO data\n",
    "    \"\"\"\n",
    "    # Convert NumPy arrays to Tensors\n",
    "    wt_data = torch.tensor(data_dict['wt_data'], dtype=torch.float32)\n",
    "    wt_labels = torch.tensor(data_dict['wt_labels'], dtype=torch.long)\n",
    "    ko_data = torch.tensor(data_dict['ko_data'], dtype=torch.float32)\n",
    "    ko_labels = torch.tensor(data_dict['ko_labels'], dtype=torch.long)\n",
    "    \n",
    "    # Create datasets\n",
    "    wt_dataset = TensorDataset(wt_data, wt_labels)\n",
    "    ko_dataset = TensorDataset(ko_data, ko_labels)\n",
    "    \n",
    "    # Create data loaders\n",
    "    wt_loader = DataLoader(\n",
    "        wt_dataset, \n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        drop_last=False\n",
    "    )\n",
    "    \n",
    "    ko_loader = DataLoader(\n",
    "        ko_dataset, \n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        drop_last=False\n",
    "    )\n",
    "    \n",
    "    return wt_loader, ko_loader\n",
    "\n",
    "# Set batch size (adjust based on dataset size)\n",
    "batch_size = 256\n",
    "wt_loader, ko_loader = create_dataloaders(data_dict, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5317bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 7. Initialize and Train Model\n",
    "# =============================================================================\n",
    "\n",
    "# Model parameters\n",
    "input_dim = data_dict['wt_data'].shape[1]  # Number of genes\n",
    "latent_dim = 10  # Latent space dimensions (adjustable)\n",
    "hidden_dims = (256, 128, 64)  # Hidden layer dimensions (adjustable)\n",
    "\n",
    "# Initialize model\n",
    "model = WTKOContrastiveVAE(\n",
    "    input_dim=input_dim,\n",
    "    latent_dim=latent_dim, \n",
    "    hidden_dims=hidden_dims,\n",
    "    tau=0.3,  # Temperature parameter\n",
    "    lambda_contrast=10,  # Contrastive loss weight\n",
    "    lambda_align=10,  # Cluster alignment loss weight\n",
    "    dropout_prob=0.2,  # Dropout probability\n",
    "    norm_type='batch'  # Normalization type\n",
    ")\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = WTKOTrainer(model)\n",
    "\n",
    "# Train model\n",
    "print(\"Starting model training...\")\n",
    "history = trainer.train(\n",
    "    wt_loader=wt_loader,\n",
    "    ko_loader=ko_loader,\n",
    "    num_epochs=400,  # Number of epochs\n",
    "    lr=1e-3,  # Learning rate\n",
    "    weight_decay=1e-3,  # Weight decay\n",
    "    save_path='./models/wtko_vae',  # Model save path (adjust as needed)\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88349361",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 8. Extract Latent Representations and Visualize\n",
    "# =============================================================================\n",
    "\n",
    "# Get latent representations from trained model\n",
    "wt_latent, wt_labels = trainer.get_latent_representations(wt_loader)\n",
    "ko_latent, ko_labels = trainer.get_latent_representations(ko_loader)\n",
    "\n",
    "# Convert to numpy arrays\n",
    "wt_latent_np = wt_latent.cpu().numpy()\n",
    "ko_latent_np = ko_latent.cpu().numpy()\n",
    "wt_labels_np = wt_labels.cpu().numpy()\n",
    "ko_labels_np = ko_labels.cpu().numpy()\n",
    "\n",
    "# Project to 2D using UMAP\n",
    "reducer = umap.UMAP(n_neighbors=15, min_dist=0.1, n_components=2, random_state=42)\n",
    "\n",
    "# Combine WT and KO data for projection\n",
    "combined_latent = np.vstack([wt_latent_np, ko_latent_np])\n",
    "combined_embedding = reducer.fit_transform(combined_latent)\n",
    "\n",
    "# Split projection results\n",
    "wt_embedding = combined_embedding[:len(wt_latent_np)]\n",
    "ko_embedding = combined_embedding[len(wt_latent_np):]\n",
    "\n",
    "# Cell type names list\n",
    "cell_type_names = data_dict['cell_types']\n",
    "\n",
    "# =============================================================================\n",
    "# 9. Visualization Functions and Plots\n",
    "# =============================================================================\n",
    "\n",
    "def plot_latent_space(wt_embedding, ko_embedding, wt_labels, ko_labels, cell_type_names):\n",
    "    \"\"\"\n",
    "    Visualize latent space projections\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    wt_embedding, ko_embedding : ndarray\n",
    "        UMAP embeddings for WT and KO cells\n",
    "    wt_labels, ko_labels : ndarray\n",
    "        Cell type labels\n",
    "    cell_type_names : list\n",
    "        List of cell type names\n",
    "    \"\"\"\n",
    "    # Set up color map\n",
    "    n_cell_types = len(cell_type_names)\n",
    "    colors = plt.cm.tab10(np.linspace(0, 1, n_cell_types))\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 8))\n",
    "    \n",
    "    # Visualize WT cells\n",
    "    for i, ct in enumerate(range(n_cell_types)):\n",
    "        mask = wt_labels == ct\n",
    "        if mask.sum() > 0:\n",
    "            ax1.scatter(\n",
    "                wt_embedding[mask, 0], \n",
    "                wt_embedding[mask, 1],\n",
    "                c=[colors[i]],\n",
    "                label=cell_type_names[ct],\n",
    "                s=50,\n",
    "                alpha=0.7\n",
    "            )\n",
    "    \n",
    "    ax1.set_title('WT Cells - Latent Space (UMAP)', fontsize=14)\n",
    "    ax1.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    ax1.grid(True, linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Visualize KO cells\n",
    "    for i, ct in enumerate(range(n_cell_types)):\n",
    "        mask = ko_labels == ct\n",
    "        if mask.sum() > 0:\n",
    "            ax2.scatter(\n",
    "                ko_embedding[mask, 0], \n",
    "                ko_embedding[mask, 1],\n",
    "                c=[colors[i]],\n",
    "                label=cell_type_names[ct],\n",
    "                s=50,\n",
    "                alpha=0.7\n",
    "            )\n",
    "    \n",
    "    ax2.set_title('KO Cells - Latent Space (UMAP)', fontsize=14)\n",
    "    ax2.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    ax2.grid(True, linestyle='--', alpha=0.7)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_combined_latent_space(wt_embedding, ko_embedding, wt_labels, ko_labels, cell_type_names):\n",
    "    \"\"\"\n",
    "    Visualize WT and KO cells in combined space (color=cell type, marker=WT/KO)\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    wt_embedding, ko_embedding : ndarray\n",
    "        UMAP embeddings for WT and KO cells\n",
    "    wt_labels, ko_labels : ndarray\n",
    "        Cell type labels\n",
    "    cell_type_names : list\n",
    "        List of cell type names\n",
    "    \"\"\"\n",
    "    n_cell_types = len(cell_type_names)\n",
    "    colors = plt.cm.tab10(np.linspace(0, 1, n_cell_types))\n",
    "\n",
    "    # Combine data\n",
    "    combined_embedding = np.vstack([wt_embedding, ko_embedding])\n",
    "    combined_labels = np.concatenate([wt_labels, ko_labels])\n",
    "    combined_group = np.array(['WT'] * len(wt_labels) + ['KO'] * len(ko_labels))\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    for i in range(n_cell_types):\n",
    "        for group, marker in zip(['WT', 'KO'], ['o', 's']):\n",
    "            mask = (combined_labels == i) & (combined_group == group)\n",
    "            if mask.sum() > 0:\n",
    "                plt.scatter(\n",
    "                    combined_embedding[mask, 0],\n",
    "                    combined_embedding[mask, 1],\n",
    "                    c=[colors[i]],\n",
    "                    label=f\"{cell_type_names[i]} ({group})\",\n",
    "                    s=50,\n",
    "                    alpha=0.7,\n",
    "                    marker=marker,\n",
    "                    edgecolor='k' if group == 'KO' else 'none',\n",
    "                    linewidths=0.5\n",
    "                )\n",
    "    \n",
    "    plt.title('WT and KO Cells - Combined Latent Space (UMAP)', fontsize=14)\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', ncol=1)\n",
    "    plt.grid(True, linestyle='--', alpha=0.5)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa51387a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate visualizations\n",
    "print(\"Generating latent space visualizations...\")\n",
    "\n",
    "# Separate WT/KO visualization\n",
    "plot_latent_space(wt_embedding, ko_embedding, wt_labels_np, ko_labels_np, cell_type_names)\n",
    "\n",
    "# Combined WT/KO visualization\n",
    "plot_combined_latent_space(\n",
    "    wt_embedding=wt_embedding,\n",
    "    ko_embedding=ko_embedding,\n",
    "    wt_labels=wt_labels_np,\n",
    "    ko_labels=ko_labels_np,\n",
    "    cell_type_names=cell_type_names\n",
    ")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Pipeline execution completed successfully!\")\n",
    "print(\"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "VC_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
